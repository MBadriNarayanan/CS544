{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Badri Narayanan Murali Krishnan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -D dfs.replication=1 -cp -f data/*.csv hdfs://nn:9000/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"cs544\")\n",
    "    .master(\"spark://boss:7077\")\n",
    "    .config(\"spark.executor.memory\", \"512M\")\n",
    "    .config(\"spark.sql.warehouse.dir\", \"hdfs://nn:9000/user/hive/warehouse\")\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#q1\n",
    "banks_df = spark.read.csv(\n",
    "    \"hdfs://nn:9000/arid2017_to_lei_xref_csv.csv\", header=True, inferSchema=True\n",
    ")\n",
    "count = banks_df.rdd.filter(\n",
    "    lambda row: row[\"respondent_name\"].startswith(\"The\")\n",
    ").count()\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banks_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#q2\n",
    "count = banks_df.filter(\"\"\"respondent_name LIKE 'The%'\"\"\").count()\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#q3\n",
    "banks_df.write.saveAsTable(\"banks\", mode=\"overwrite\")\n",
    "sql_table = spark.sql(\n",
    "    \"\"\"SELECT COUNT(*) as count FROM banks WHERE respondent_name LIKE 'The%'\"\"\"\n",
    ")\n",
    "count = sql_table.collect()[0][\"count\"]\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_df = spark.read.csv(\n",
    "    \"hdfs://nn:9000/hdma-wi-2021.csv\", header=True, inferSchema=True\n",
    ")\n",
    "loans_df.write.bucketBy(8, \"county_code\").saveAsTable(\"loans\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_names = [\n",
    "    \"ethnicity\",\n",
    "    \"race\",\n",
    "    \"sex\",\n",
    "    \"states\",\n",
    "    \"counties\",\n",
    "    \"tracts\",\n",
    "    \"action_taken\",\n",
    "    \"denial_reason\",\n",
    "    \"loan_type\",\n",
    "    \"loan_purpose\",\n",
    "    \"preapproval\",\n",
    "    \"property_type\",\n",
    "]\n",
    "for view_name in view_names:\n",
    "    df = spark.read.csv(\n",
    "        f\"hdfs://nn:9000/{view_name}.csv\", header=True, inferSchema=True\n",
    "    )\n",
    "    df.createOrReplaceTempView(view_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#q4\n",
    "tables_df = spark.sql(\"SHOW TABLES\")\n",
    "tables_dict = {row[\"tableName\"]: row[\"isTemporary\"] for row in tables_df.collect()}\n",
    "tables_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#q5\n",
    "query = \"\"\"\n",
    "SELECT COUNT(*) as count \n",
    "FROM banks INNER JOIN loans ON banks.lei_2020 = loans.lei \n",
    "WHERE banks.respondent_name = 'University of Wisconsin Credit Union'\n",
    "\"\"\"\n",
    "result = spark.sql(query)\n",
    "count = result.collect()[0][\"count\"]\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#q6\n",
    "result.explain(\"formatted\")\n",
    "# 1. The banks table gets broadcast to all executors. This means\n",
    "#    Spark first filters the banks table to just find UWCU's records, then copies this small\n",
    "#    filtered subset to every executor. This is much more efficient than shuffling around the\n",
    "#    full loans table or trying to coordinate joins across nodes.\n",
    "#\n",
    "# 2. Looking at HashAggregates in the plan:\n",
    "#    - First HashAggregate: Each executor counts its own portion of matching loans locally\n",
    "#      (this is labeled as \"partial\" in the plan)\n",
    "#    - Second HashAggregate: All these local counts get combined into one final number\n",
    "#      (this is labeled as \"final\" in the plan)\n",
    "#    Similar to divide and conquer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#q7\n",
    "query = \"\"\"\n",
    "WITH stats AS (\n",
    "    SELECT c.NAME as county_name, COUNT(*) as application_count, AVG(l.interest_rate) as avg_interest_rate\n",
    "    FROM loans l\n",
    "    INNER JOIN banks b ON l.lei = b.lei_2020\n",
    "    INNER JOIN counties c ON l.county_code = c.STATE * 1000 + c.COUNTY\n",
    "    WHERE b.respondent_name = \"Wells Fargo Bank, National Association\"\n",
    "    GROUP BY c.NAME\n",
    ")\n",
    "SELECT county_name, avg_interest_rate\n",
    "FROM stats\n",
    "ORDER BY application_count DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "results = spark.sql(query).collect()\n",
    "county_rates = {row[\"county_name\"]: row[\"avg_interest_rate\"] for row in results}\n",
    "counties = list(county_rates.keys())\n",
    "rates = list(county_rates.values())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(counties, rates)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Average Interest Rate\")\n",
    "plt.xlabel(\"County\")\n",
    "plt.title(\"Average Wells Fargo Interest Rates by County\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../q7.png\")\n",
    "\n",
    "county_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#q8\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT county_code, AVG(interest_rate) as avg_rate\n",
    "    FROM loans\n",
    "    GROUP BY county_code\n",
    "    \"\"\"\n",
    ").explain()\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT lei, AVG(interest_rate) as avg_rate\n",
    "    FROM loans\n",
    "    GROUP BY lei\n",
    "    \"\"\"\n",
    ").explain()\n",
    "# Network I/O between partial_mean and mean operations is required when:\n",
    "# 1. The data for a single group is spread across multiple partitions\n",
    "# 2. We need to combine partial results from different executors to get the final mean\n",
    "#\n",
    "# GROUP BY county_code (No Network I/O needed):\n",
    "# - When we created the loans table, we used bucketBy(8, \"county_code\"), \n",
    "#   effectively pre-sorting the data so all loans for each county are in the same bucket.\n",
    "# - This setup allows each executor to calculate means for counties independently, \n",
    "#   as it has all necessary data locally. There's no need to exchange data across the network.\n",
    "\n",
    "#\n",
    "# GROUP BY lei (Network I/O required):\n",
    "# - We didn't bucketize by lei, so loan applications for each bank are randomly scattered\n",
    "#   across our 8 buckets, randomly filed across different drawers\n",
    "# - To calculate a bank's mean interest rate, we need to:\n",
    "#   1. Each executor calculates partial means for its piece (partial_mean)\n",
    "#   2. Shuffle data over network to group all pieces for each bank together\n",
    "#   3. Combine these partial results into final means (mean)\n",
    "#\n",
    "# The execution plans confirm this: county_code grouping shows a simpler plan without Exchange\n",
    "# (shuffle) operations, while lei grouping requires data Exchange to compute accurate means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#q9\n",
    "df = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "        CAST(loan_amount AS DOUBLE) AS loan_amount,\n",
    "        CAST(income AS DOUBLE) AS income,\n",
    "        CAST(interest_rate AS DOUBLE) AS interest_rate,\n",
    "        CASE WHEN action_taken = 1 THEN 1.0 ELSE 0.0 END AS approval\n",
    "    FROM loans\n",
    "    \"\"\"\n",
    ")\n",
    "df = df.select(\"loan_amount\", \"income\", \"interest_rate\", \"approval\").fillna(0.0)\n",
    "train, test = df.randomSplit([0.8, 0.2], seed=41)\n",
    "train.cache()\n",
    "\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"loan_amount\", \"income\", \"interest_rate\"], outputCol=\"X\"\n",
    ")\n",
    "\n",
    "train_data = assembler.transform(train)\n",
    "test_data = assembler.transform(test)\n",
    "\n",
    "accuracy = {\"depth=1\": 1, \"depth=5\": 5, \"depth=10\": 10, \"depth=15\": 15, \"depth=20\": 20}\n",
    "\n",
    "for key, depth in accuracy.items():\n",
    "    decision_tree = DecisionTreeClassifier(\n",
    "        maxDepth=depth, labelCol=\"approval\", featuresCol=\"X\", seed=41\n",
    "    )\n",
    "    classifier = decision_tree.fit(train_data)\n",
    "    y_pred = classifier.transform(test_data)\n",
    "    total = y_pred.filter(\n",
    "        (y_pred.approval.isNotNull()) & (y_pred.prediction.isNotNull())\n",
    "    ).count()\n",
    "    correct = y_pred.filter(\n",
    "        (y_pred.approval == y_pred.prediction) & (y_pred.approval.isNotNull())\n",
    "    ).count()\n",
    "    accuracy[key] = float(correct / total)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q10\n",
    "# No, the test accuracy does not always increase with larger max_depth values.\n",
    "# Looking at the numbers:\n",
    "# - Accuracy increases from depth 1 to 10\n",
    "# - But then starts decreasing after depth 10\n",
    "# - At depth 20, accuracy is lower than at depth 10\n",
    "#\n",
    "# This happens because deeper trees can start \"memorizing\" the training data\n",
    "# rather than learning general patterns => Overfitting.\n",
    "# At some point, making the tree deeper just makes it overfit to the training\n",
    "# data, hurting its ability to make good predictions on new data it hasn't\n",
    "# seen before."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
